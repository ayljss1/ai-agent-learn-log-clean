# rag_api.pyï¼ˆDay 4 æ”¹è¿›ç‰ˆ + config + rebuild æ”¯æŒ + æ—¥å¿—ï¼‰

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import requests
import os
import numpy as np
from sentence_transformers import SentenceTransformer
from config import DATA_DIR, INDEX_PATH, META_PATH, EMBEDDING_MODEL_NAME, LLM_MODEL_NAME, OLLAMA_API_URL, TOP_K
from rag_utils import (
    load_all_paragraphs_from_folder,
    embed_paragraphs,
    build_faiss_index,
    save_faiss_index,
    load_faiss_index,
    save_paragraphs,
    load_paragraphs_json,
    search_similar,
    build_prompt
)

# åº”ç”¨
app = FastAPI()
model = SentenceTransformer(EMBEDDING_MODEL_NAME)

# å…¨å±€æ•°æ®
index = None
paragraphs = []

def build_or_load_index(force_rebuild=False):
    global index, paragraphs
    if not force_rebuild and os.path.exists(INDEX_PATH) and os.path.exists(META_PATH):
        print("ğŸ“‚ æ­£åœ¨åŠ è½½å·²æœ‰ç´¢å¼•...")
        index = load_faiss_index(INDEX_PATH)
        paragraphs = load_paragraphs_json(META_PATH)
    else:
        print("ğŸ”§ æ­£åœ¨é‡æ–°æ„å»ºç´¢å¼•...")
        paragraphs = load_all_paragraphs_from_folder(DATA_DIR)
        print(f"ğŸ“ƒ åŠ è½½æ®µè½æ•°ï¼š{len(paragraphs)}")
        embeddings = embed_paragraphs(model, paragraphs)
        index = build_faiss_index(embeddings)
        save_faiss_index(index, INDEX_PATH)
        save_paragraphs(paragraphs, META_PATH)
        print("âœ… ç´¢å¼•æ„å»ºå¹¶ä¿å­˜æˆåŠŸï¼")

# åˆå§‹åŒ–ä¸€æ¬¡
build_or_load_index()

# æ¥å£æ¨¡å‹
class QuestionRequest(BaseModel):
    question: str

@app.post("/ask")
def ask(req: QuestionRequest):
    try:
        q_vector = model.encode([req.question])
        distances, indices = search_similar(index, q_vector, top_k=TOP_K)
        top_context = [paragraphs[i] for i in indices]
        prompt = build_prompt(top_context, req.question)

        print("\nğŸ’¬ æé—®ï¼š", req.question)
        print("ğŸ“š æ£€ç´¢æ®µè½ï¼š")
        for i, p in enumerate(top_context):
            print(f"Top {i+1}: {p}")

        response = requests.post(OLLAMA_API_URL, json={
            "model": LLM_MODEL_NAME,
            "prompt": prompt,
            "stream": False
        })
        result = response.json()
        return {"answer": result["response"]}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/rebuild")
def rebuild():
    try:
        build_or_load_index(force_rebuild=True)
        return {"message": f"ç´¢å¼•å·²æˆåŠŸé‡å»ºï¼Œæ®µè½æ•°é‡ï¼š{len(paragraphs)}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é‡å»ºå¤±è´¥: {str(e)}")