import faiss
from sentence_transformers import SentenceTransformer
import os

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
data_path = os.path.join(project_root, "data/sample.txt")

# 1. åŠ è½½æ®µè½
with open(data_path, "r", encoding="utf-8") as f:
    text = f.read()
paragraphs = [p.strip() for p in text.split("ã€‚") if p.strip()]

# 2. åŠ è½½ FAISS å‘é‡åº“
index = faiss.read_index("day02/faiss_index.idx")

# 3. åŠ è½½æ¨¡å‹
model = SentenceTransformer("all-MiniLM-L6-v2")

# 4. è¾“å…¥é—®é¢˜
query = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
query_embedding = model.encode([query])

# 5. å‘é‡æœç´¢
k = 3
distances, indices = index.search(query_embedding, k)

# 6. è¾“å‡ºæœ€ç›¸å…³æ®µè½
print("\nğŸ“š ä¸é—®é¢˜æœ€ç›¸å…³çš„æ®µè½ï¼š")
for i, idx in enumerate(indices[0]):
    print(f"Top {i+1}: {paragraphs[idx]}  (è·ç¦»: {distances[0][i]:.4f})")
